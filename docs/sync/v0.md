# Sync Protocol v0 (Draft)

This document describes a minimal, transport-agnostic synchronization protocol for TreeCRDT operation logs.

## Goals
- Allow two peers who have been offline to efficiently exchange missing operations and converge.
- Work over any transport (libp2p, WebSocket, HTTP, etc).
- Be language/runtime neutral (TypeScript today; Rust later).

## Non-goals (v0)
- Encryption / E2EE / group key management.
- ACL rules, signature verification, or identity management beyond stable `replicaId`s.
- Partial sync (subtree filters), snapshots, compression, or advanced anti-entropy.

## Background

TreeCRDT operations are uniquely identified by `(replicaId, counter)` (a per-replica monotonic counter).
This supports a **version vector** approach: for each replica, the peer tracks the maximum counter it has seen.

Lamport timestamps are useful for deterministic ordering and clock observation, but **Lamport alone is not sufficient**
for completeness in a p2p setting (two offline peers can generate overlapping Lamport ranges independently).

## Terms
- `docId`: document identifier / namespace used to avoid mixing ops from different documents.
- `replicaId`: stable identifier for an author/device/replica. Treated as opaque bytes.
- `counter`: monotonically increasing per `replicaId`.
- `opId`: `(replicaId, counter)`.
- `heads` / `versionVector`: mapping `replicaId -> max counter this peer has`.
- `lamport`: logical timestamp carried by each op for ordering and clock observation.

## Encoding

This spec defines message *fields* and *semantics*. The wire encoding is transport-specific.

- Examples in this doc use JSON-like objects for readability.
- Real implementations SHOULD use a canonical binary encoding (CBOR / protobuf) so that:
  - message sizes are smaller, and
  - it is easy to define canonical "bytes-to-sign" later for authenticated sync.

## Message types

All messages MUST include:
- `type`: discriminant
- `v`: protocol version (`0`)
- `docId`

### 1) `Have`
Announces local heads (version vector).

Example:
```json
{
  "type": "have",
  "v": 0,
  "docId": "my-doc",
  "heads": { "replicaA": 12, "replicaB": 4 },
  "maxLamport": 42
}
```

Notes:
- `maxLamport` is optional but recommended so receivers can update their clocks (observe) before generating new local ops.
- A peer SHOULD send `Have` on connect and periodically during long sessions (anti-entropy heartbeat).

### 2) `RequestOps`
Requests missing ops from another peer.

Example:
```json
{
  "type": "request_ops",
  "v": 0,
  "docId": "my-doc",
  "want": [
    { "replicaId": "replicaA", "fromCounterExclusive": 7 },
    { "replicaId": "replicaB", "fromCounterExclusive": 0 }
  ],
  "limitOps": 500,
  "cursor": null
}
```

Semantics:
- For each entry, the responder should return ops where:
  - `op.replicaId == replicaId` AND `op.counter > fromCounterExclusive`,
  - ordered by `counter` ascending.
- `limitOps` is a responder-facing hint for batching. Implementations SHOULD also enforce a max-bytes limit.
- `cursor` is optional and can be used by a responder to paginate large catch-ups.

### 3) `OpsBatch`
Returns a batch of operations.

Example:
```json
{
  "type": "ops_batch",
  "v": 0,
  "docId": "my-doc",
  "ops": [ /* operations */ ],
  "cursor": "opaque-next-page-token",
  "done": false
}
```

Semantics:
- `ops` contains a list of operations encoded according to `@treecrdt/interface` (or the canonical protocol encoding).
- `cursor` and `done` indicate whether more ops remain for the previous request.
- Receivers MUST treat ops as idempotent: duplicates and replays are expected in p2p.

### 4) `Error`
Signals request failures.

Example:
```json
{
  "type": "error",
  "v": 0,
  "docId": "my-doc",
  "code": "unsupported_version",
  "message": "peer only supports v0"
}
```

## Catch-up algorithm (sync once)

1) **Handshake**
   - A → B: `Have(A.heads, A.maxLamport)`
   - B → A: `Have(B.heads, B.maxLamport)`
   - Each side observes the other side’s `maxLamport` before generating new local ops.

2) **Compute missing ranges**
   - For each replica `r` known to either side:
     - If `B.heads[r] > A.heads[r]` then A is missing `(r, A.heads[r] + 1 .. B.heads[r])`.
     - Unknown replicas are treated as `0`.

3) **Request + batch transfer**
   - A → B: `RequestOps` for replicas where A is behind.
   - B → A: `OpsBatch` (possibly multiple pages) until `done: true`.
   - Repeat symmetrically for B.

4) **Apply**
   - Receiver stores ops idempotently (dedupe by `(replicaId, counter)`).
   - Receiver applies ops in deterministic order; typical ordering is `(lamport, replicaId, counter)`.

5) **Convergence**
   - If ops are still being generated during the exchange, do another round:
     - send new `Have`, compute diffs again, repeat.

## Worked example

Initial state:
- Peer A heads: `{ A: 3 }`
- Peer B heads: `{ A: 1, B: 2 }`

On connect:
- Exchange `Have`.
- A is missing `B: (0 .. 2]`.
- B is missing `A: (1 .. 3]`.

Then:
- A → B: `RequestOps(want: [{replicaId: "B", fromCounterExclusive: 0}])`
- B → A: `OpsBatch(ops: B#1, B#2, done: true)`
- B → A: `RequestOps(want: [{replicaId: "A", fromCounterExclusive: 1}])`
- A → B: `OpsBatch(ops: A#2, A#3, done: true)`

Both peers now have heads `{ A: 3, B: 2 }` and converge.

